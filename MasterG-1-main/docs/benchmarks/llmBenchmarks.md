# Technical Evaluation: Small Language Models (SLM) for On-Device Reasoning

## 1. Executive Summary

This report benchmarks four Small Language Models (SLMs) to determine the optimal engine for a local content generation and reasoning system. The evaluation prioritizes **reasoning capability (Chain of Thought)**, **factual adherence**, and **auditable logic** over raw inference speed.

**Final Recommendation:** **DeepSeek-R1 1.5B** is selected as the production model. It demonstrates a superior balance of reasoning density and memory efficiency, outperforming larger general-purpose models in logical consistency while maintaining a sub-1.5GB footprint.

## 2. Evaluation Scope & Criteria

The models were evaluated in a constrained offline environment (CPU Inference).

* **DeepSeek-R1 1.5B:** Reasoning-focused model designed for strong chain-of-thought performance.
* **Llama 3.2 3B (Instruct):** Meta‚Äôs latest optimized instruction-following model (Quantized).
* **Gemma 2 2B (Instruct):** Google‚Äôs lightweight open model.
* **Qwen 2.5 0.5B:** A nano-sized general model for extreme resource constraints.

**Key Metrics:**

* **Reasoning Efficacy:** Zero-shot performance on multi-step logic tasks.
* **Memory Footprint:** VRAM/RAM usage during context loading.
* **Auditability:** Availability of "Thinking" process logs (CoT traces).

## 3. Comparative Benchmark Data

| Model | Variant | Disk Size (Quantized) | Est. RAM Usage | Reasoning Score (Est. GSM8K) | Inference Speed (CPU) | Primary Architecture |
| --- | --- | --- | --- | --- | --- | --- |
| **DeepSeek-R1** | 1.5B | **~1.1 GB** | ~2.2 GB | **High (~78%)** | 35-45 t/s | **Reasoning (CoT)** |
| **Llama 3.2** | 3B Instruct | ~2.4 GB | ~4.1 GB | Med-High (~72%) | 15-25 t/s | General Purpose |
| **Gemma 2** | 2B Instruct | ~1.6 GB | ~3.0 GB | Moderate (~55%) | 25-30 t/s | General Purpose |
| **Qwen 2.5** | 0.5B Instruct | ~0.4 GB | ~0.8 GB | Low (<40%) | **60+ t/s** | General Purpose |

> *Note: Disk sizes assume 4-bit (Q4_K_M) quantization via GGUF format.*

## 4. Deep Dive Analysis

### ‚úÖ Winner: DeepSeek-R1 1.5B

**Classification:** Specialized Reasoning Model

**Analysis:**

Unlike standard LLMs that predict the next token immediately, DeepSeek-R1 is trained via Reinforcement Learning to generate "thinking traces" before answering.

**Pros:**
* **Auditable Intelligence:** The model outputs a reasoning block, allowing developers to debug why a hallucination or logic error occurred.
* **Efficiency:** It achieves reasoning parity with ~7B parameter models while using only 1.5B parameters.
* **Stability:** High resistance to prompt drift during long-context educational tasks.

**Cons:**
* Slightly higher latency due to the extra tokens generated during the "thinking" phase.



### ‚ö†Ô∏è Llama 3.2 3B (Instruct)

**Classification:** General Purpose / Instruction Tuned

**Analysis:**
Llama 3.2 is a robust "jack-of-all-trades." It has excellent language fluency and instruction following but lacks the dedicated internal monologue training of R1.

**Verdict:** **Impractical for this specific constraint.** While capable, the 2.4GB+ size and slower CPU inference do not justify the marginal gain in fluency over DeepSeek-R1 for this specific use case.

### ‚ùå Gemma 2 2B

**Classification:** Lightweight General LLM

**Analysis:**
Gemma 2 2B is highly capable for creative writing but struggles with strict logical adherence in educational contexts.

**Verdict:** **Unreliable for Logic.** Exhibits a high "confidence-to-accuracy" gap (hallucinates confidently). Best reserved for creative summarization, not logical deduction.

### ‚ùå Qwen 2.5 0.5B

**Classification:** Nano LLM

**Analysis:**

An engineering marvel for its size (<1GB RAM), but insufficient for complex reasoning.

**Verdict:** **Too Small.** The parameter count is too low to sustain consistent factual accuracy for educational content.

## 5. Decision Matrix

The decision is driven by the **Reasoning-to-RAM Ratio**:

| Feature Requirement | DeepSeek-R1 (1.5B) | Llama 3.2 (3B) | Gemma 2 (2B) |
| --- | --- | --- | --- |
| **Strict Logical Reasoning** | üü¢ **Best** | üü° Good | üî¥ Weak |
| **Traceable "Thinking" Logs** | üü¢ **Yes** | üî¥ No | üî¥ No |
| **Low RAM (<3GB)** | üü¢ **Yes** | üî¥ No | üü¢ Yes |
| **Hallucination Resistance** | üü¢ **High** | üü° Medium | üî¥ Low |

## 6. Conclusion

**DeepSeek-R1 1.5B** is the only model that satisfies the project's requirement for auditable reasoning within the hardware constraints. Its ability to self-correct via Chain-of-Thought (CoT) processing makes it uniquely suited for the content generation layer, minimizing the risk of unverified hallucinations common in standard small language models.

**Next Step:** Proceed with full integration of the DeepSeek-R1 1.5B GGUF (Q4_K_M) variant.