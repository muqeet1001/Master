# ================================
# EduRAG Backend Environment Variables
# ================================

# --------------------------------
# Server Configuration
# --------------------------------
PORT=5001
NODE_ENV=development

# --------------------------------
# Database Configuration
# --------------------------------

# MongoDB Connection URI
# Local: mongodb://localhost:27017/edurag
# Atlas: mongodb+srv://<username>:<password>@cluster.mongodb.net/edurag
MONGODB_URI=mongodb://localhost:27017/edurag

# ChromaDB Server URL
# Default local: http://localhost:8000
# Docker: http://chromadb:8000
CHROMA_URL=http://localhost:8000

# --------------------------------
# AI API Keys
# --------------------------------

# Groq API Key (for fast LLM - Layer 1 routing)
# Get from: https://console.groq.com/keys
# Format: gsk_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
GROQ_API_KEY=your_groq_api_key_here

# Google Gemini API Key (for deep understanding - Layer 3)
# Get from: https://aistudio.google.com/app/apikey
# Format: AIzaSyxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
# Note: This is labeled as GEMMA_API_KEY in code for legacy reasons
GEMMA_API_KEY=your_gemini_api_key_here

# HuggingFace API Key (for text embeddings)
# Get from: https://huggingface.co/settings/tokens
# Format: hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx
HUGGINGFACE_API_KEY=your_huggingface_api_key_here

# --------------------------------
# Important Notes
# --------------------------------
# 1. NEVER commit .env file with real API keys
# 2. Create a copy as .env and add your actual keys
# 3. Keep API keys secure and rotate them regularly
# 4. Use different keys for development and production

# ================================
# OFFLINE MODE CONFIGURATION
# ================================

# Set to 'offline' to use Ollama (no internet required)
# Set to 'online' to use Groq/Gemini APIs
# Set to 'auto' to automatically detect and prefer offline if available
USE_OFFLINE_MODE=offline

# --------------------------------
# Ollama Configuration (for offline mode)
# --------------------------------

# Ollama server URL (default: http://localhost:11434)
OLLAMA_URL=http://localhost:11434

# Chat/Reasoning model (DeepSeek R1)
OLLAMA_CHAT_MODEL=deepseek-r1:1.5b

# Embedding model
OLLAMA_EMBED_MODEL=embeddinggemma:latest

# --------------------------------
# Ollama Setup Instructions
# --------------------------------
# 1. Install Ollama from https://ollama.ai/download
# 2. Start Ollama: `ollama serve`
# 3. Pull required models:
#    - ollama pull deepseek-r1:1.5b
#    - ollama pull embeddinggemma:latest (or your custom model)
# 4. Set USE_OFFLINE_MODE=offline in .env

# ================================
# FILE STORAGE CONFIGURATION
# ================================

# Directory for storing uploaded files (for preview functionality)
# In Docker: mount this as a volume for persistence
# Default: ./uploads/files (relative to backend root)
FILE_STORAGE_DIR=./uploads/files

# For Docker/containerized deployment, use absolute path:
# FILE_STORAGE_DIR=/app/uploads/files
# And mount a volume: -v /host/path/to/uploads:/app/uploads/files

# ================================
# NLLB-200 TRANSLATION CONFIGURATION
# ================================

# Enable/disable NLLB-200 translation service
# Set to 'true' to enable offline translation for 200+ languages
# Set to 'false' to disable (will use fallback translation service)
NLLB_ENABLED=true

# Python executable path (for NLLB proxy server)
# Default: python (uses system Python)
# For virtual environment: ./proxy/venv/Scripts/python.exe (Windows)
# For virtual environment: ./proxy/venv/bin/python (Linux/Mac)
PYTHON_EXECUTABLE=python

# Custom model directory (optional)
# Default: ./proxy/models
# NLLB_MODELS_DIR=./proxy/models

# Custom model path (optional)
# Default: ./proxy/models/nllb-200-distilled-600M
# NLLB_MODEL_PATH=./proxy/models/nllb-200-distilled-600M

# --------------------------------
# NLLB Setup Instructions
# --------------------------------
# 1. Navigate to backend/proxy directory
# 2. Run setup script:
#    Windows: setup_nllb.bat
#    Linux/Mac: ./setup_nllb.sh
# 3. Wait for model download (~2.4GB, one-time only)
# 4. Set NLLB_ENABLED=true in .env
# 5. Start backend server - NLLB will start automatically
#
# After setup, NLLB runs completely offline (no internet required)
# Supports 200+ languages with high-quality neural translation
